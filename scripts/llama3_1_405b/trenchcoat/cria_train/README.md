
Here we provide 14 scripts, each of which performs attention distillation on a Cria of 9 layers. Recall that a baby Llama is called a Cria. 

A training run for 9 layers fits on one 80GB GPU. 
